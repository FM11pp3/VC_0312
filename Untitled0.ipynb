{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cf3d4d5",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FM11pp3/VC_0312/blob/main/Untitled0.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VC_0312 - Notebook arrumado\n",
    "Notebook dividido em: Configuracao -> Parte A (analise exploratoria + augmentations) -> Parte B (pesos pre-treinados para validar/testar) -> Anexos (treino + push GitHub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como correr**\n",
    "- Ajusta `DATA_DIR` se nao estiveres em Colab.\n",
    "- Executa as celulas por ordem: Configuracao -> Parte A -> Parte B.\n",
    "- As celulas de Anexos sao opcionais para treino de raiz e push."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 42\n",
    "\n",
    "def seed_everything(seed: int = SEED) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything()\n",
    "print(f\"Device: {DEVICE} | Seed: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos principais e dataset\n",
    "REPO_ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = Path(\"/content/InfraredSolarModules\") if Path(\"/content\").exists() else REPO_ROOT / \"InfraredSolarModules\"\n",
    "DATA_URL = \"https://github.com/RaptorMaps/InfraredSolarModules/raw/master/2020-02-14_InfraredSolarModules.zip\"\n",
    "BASE_IMAGE_DIR = DATA_DIR / \"images\"\n",
    "MODELS_DIR = REPO_ROOT / \"models\"\n",
    "METRICS_DIR = REPO_ROOT / \"metrics\"\n",
    "TRAIN_CSV = REPO_ROOT / \"full_train_data_list.csv\"\n",
    "TEST_CSV = REPO_ROOT / \"final_test_data_list.csv\"\n",
    "\n",
    "def ensure_dataset() -> None:\n",
    "    \"\"\"Descarrega o dataset apenas se nao existir localmente.\"\"\"\n",
    "    if BASE_IMAGE_DIR.exists():\n",
    "        print(f\"?? Dataset pronto em {BASE_IMAGE_DIR}\")\n",
    "        return\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    zip_path = DATA_DIR.with_suffix(\".zip\")\n",
    "    print(\"?? A descarregar InfraredSolarModules (pode demorar)...\")\n",
    "    urllib.request.urlretrieve(DATA_URL, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(DATA_DIR.parent)\n",
    "    print(f\"?? Dataset extraido para {DATA_DIR}\")\n",
    "\n",
    "def load_dataframes(image_dir: Path = BASE_IMAGE_DIR):\n",
    "    train_df = pd.read_csv(TRAIN_CSV)\n",
    "    test_df = pd.read_csv(TEST_CSV)\n",
    "    for df in (train_df, test_df):\n",
    "        df[\"filename\"] = df[\"path\"].apply(lambda p: Path(p).name)\n",
    "        df[\"path\"] = df[\"filename\"].apply(lambda n: image_dir / n)\n",
    "    class_pairs = train_df[[\"class_name\", \"label\"]].drop_duplicates().sort_values(\"label\")\n",
    "    classes_map = {row.class_name: int(row.label) for row in class_pairs.itertuples()}\n",
    "    idx_to_class = {v: k for k, v in classes_map.items()}\n",
    "    return train_df, test_df, classes_map, idx_to_class\n",
    "\n",
    "ensure_dataset()\n",
    "train_df, test_df, classes_map, idx_to_class = load_dataframes()\n",
    "print(f\"Train imgs: {len(train_df):,} | Test imgs: {len(test_df):,}\")\n",
    "display(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte A - Analise exploratoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuicao de classes (dataset original esta desbalanceado)\n",
    "order = train_df[\"class_name\"].value_counts().index\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.countplot(data=train_df, y=\"class_name\", order=order, palette=\"viridis\", ax=ax)\n",
    "ax.set_title(\"Distribuicao de classes (train)\")\n",
    "ax.bar_label(ax.containers[0], fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar imagem original + transformer base (sem flips/rotacoes)\n",
    "sample = train_df.sample(1, random_state=SEED).iloc[0]\n",
    "img_path = Path(sample[\"path\"])\n",
    "if not img_path.exists():\n",
    "    raise FileNotFoundError(f\"Imagem nao encontrada: {img_path}. Confirma a celula de download do dataset.\")\n",
    "\n",
    "img = Image.open(img_path).convert(\"L\")\n",
    "preview_size = (64, 64)\n",
    "preview_transform = T.Compose([\n",
    "    T.Resize(preview_size),\n",
    "    T.Grayscale(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "transformed = preview_transform(img)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(img, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow((transformed.squeeze(0) * 0.5 + 0.5).clamp(0, 1), cmap=\"gray\")\n",
    "axes[1].set_title(\"Transformer base (sem flips/rotacoes)\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Classe: {sample['class_name']} | ficheiro: {img_path.name}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer base (sem flips/rotacoes) usado nos DataLoaders\n",
    "IMAGE_SIZE = (64, 64)\n",
    "base_transform = T.Compose([\n",
    "    T.Resize(IMAGE_SIZE),\n",
    "    T.Grayscale(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "train_transform = base_transform\n",
    "test_transform = base_transform\n",
    "print(\"Transformers definidos sem augmentations aleatorias.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte B - Pesos pre-treinados: validar/testar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garantir que os pesos dos modelos estao disponiveis\n",
    "WEIGHT_URLS = {\n",
    "    \"model_A_final.pth\": \"https://raw.githubusercontent.com/FM11pp3/VC_0312/main/models/model_A_final.pth\",\n",
    "    \"model_B_final.pth\": \"https://raw.githubusercontent.com/FM11pp3/VC_0312/main/models/model_B_final.pth\",\n",
    "    \"model_C_final.pth\": \"https://raw.githubusercontent.com/FM11pp3/VC_0312/main/models/model_C_final.pth\",\n",
    "}\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def ensure_weights():\n",
    "    for fname, url in WEIGHT_URLS.items():\n",
    "        dest = MODELS_DIR / fname\n",
    "        if dest.exists():\n",
    "            print(f\"?? {fname} ja existe\")\n",
    "            continue\n",
    "        print(f\"?? A descarregar {fname}...\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "    print(\"Pronto.\")\n",
    "\n",
    "ensure_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset, modelo e helpers para avaliacao\n",
    "class SolarDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"path\"]).convert(\"L\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, int(row[\"label\"])\n",
    "\n",
    "\n",
    "def make_loader(df, transform, batch_size=256, shuffle=False, sampler=None):\n",
    "    return DataLoader(\n",
    "        SolarDataset(df, transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle if sampler is None else False,\n",
    "        sampler=sampler,\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "\n",
    "class NetworkCNN(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        dummy_input = torch.randn(1, 1, 64, 64)\n",
    "        with torch.no_grad():\n",
    "            x = self.pool(F.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool(F.relu(self.conv2(x)))\n",
    "            flattened_size = torch.flatten(x, 1).shape[1]\n",
    "\n",
    "        self.fc1 = nn.Linear(flattened_size, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def evaluate_model_metrics(model: nn.Module, loader: DataLoader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            preds = model(images).argmax(1)\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "    if not all_labels:\n",
    "        return {\"accuracy\": 0.0, \"f1_macro\": 0.0, \"f1_micro\": 0.0, \"f1_weighted\": 0.0}\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "        \"f1_macro\": f1_score(all_labels, all_preds, average=\"macro\"),\n",
    "        \"f1_micro\": f1_score(all_labels, all_preds, average=\"micro\"),\n",
    "        \"f1_weighted\": f1_score(all_labels, all_preds, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            preds = model(images).argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total if total else 0.0\n",
    "\n",
    "\n",
    "def make_weighted_sampler(df: pd.DataFrame) -> WeightedRandomSampler:\n",
    "    class_counts = df[\"label\"].value_counts()\n",
    "    class_weights = 1.0 / class_counts\n",
    "    sample_weights = df[\"label\"].map(class_weights).astype(float)\n",
    "    return WeightedRandomSampler(\n",
    "        weights=torch.as_tensor(sample_weights.values, dtype=torch.double),\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Preparar dataframes para os 3 modelos\n",
    "anomaly_classes = sorted([c for c in classes_map if c != \"No-Anomaly\"])\n",
    "classes_map_B = {cls: idx for idx, cls in enumerate(anomaly_classes)}\n",
    "\n",
    "model_frames = {\n",
    "    \"A\": {\n",
    "        \"num_classes\": 2,\n",
    "        \"df\": test_df.assign(label=test_df[\"class_name\"].apply(lambda c: 0 if c == \"No-Anomaly\" else 1)),\n",
    "        \"weights\": MODELS_DIR / \"model_A_final.pth\",\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"num_classes\": len(anomaly_classes),\n",
    "        \"df\": test_df[test_df[\"class_name\"] != \"No-Anomaly\"].assign(label=lambda d: d[\"class_name\"].map(classes_map_B)),\n",
    "        \"weights\": MODELS_DIR / \"model_B_final.pth\",\n",
    "    },\n",
    "    \"C\": {\n",
    "        \"num_classes\": len(classes_map),\n",
    "        \"df\": test_df.assign(label=lambda d: d[\"class_name\"].map(classes_map)),\n",
    "        \"weights\": MODELS_DIR / \"model_C_final.pth\",\n",
    "    },\n",
    "}\n",
    "\n",
    "results = []\n",
    "for key, cfg in model_frames.items():\n",
    "    loader = make_loader(cfg[\"df\"], test_transform, batch_size=256)\n",
    "    model = NetworkCNN(cfg[\"num_classes\"]).to(DEVICE)\n",
    "    state = torch.load(cfg[\"weights\"], map_location=DEVICE)\n",
    "    model.load_state_dict(state)\n",
    "    metrics = evaluate_model_metrics(model, loader)\n",
    "    results.append({\"Model\": f\"Model {key}\", **metrics})\n",
    "    print(f\"Model {key}: acc={metrics['accuracy']:.3f} | f1_macro={metrics['f1_macro']:.3f} | f1_weighted={metrics['f1_weighted']:.3f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "metrics_path = METRICS_DIR / \"final_test_metrics.csv\"\n",
    "if metrics_path.exists():\n",
    "    print(\"Metricas exportadas no treino original:\")\n",
    "    display(pd.read_csv(metrics_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexos - Treino de raiz e push para GitHub\n",
    "Inclui treino rapido, validacao k-fold e DataLoader com WeightedRandomSampler para lidar com o desbalanceamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treino rapido (exemplo) ? usa o modelo C (12 classes) como base\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "# Variantes de treino/validacao (A/B/C padrao, D/E usam dados mix real+CGAN por defeito)\n",
    "# A: binario (No-Anomaly vs Anomaly) - real\n",
    "# B: 11 classes anomalias apenas        - real\n",
    "# C: 12 classes completas               - real\n",
    "# D: 11 classes anomalias apenas        - real + CGAN\n",
    "# E: 12 classes completas               - real + CGAN\n",
    "train_frames = {\n",
    "    \"A\": {\"num_classes\": 2, \"base\": \"A\", \"use_cgan\": False},\n",
    "    \"B\": {\"num_classes\": len(anomaly_classes), \"base\": \"B\", \"use_cgan\": False},\n",
    "    \"C\": {\"num_classes\": len(classes_map), \"base\": \"C\", \"use_cgan\": False},\n",
    "    \"D\": {\"num_classes\": len(anomaly_classes), \"base\": \"B\", \"use_cgan\": True},\n",
    "    \"E\": {\"num_classes\": len(classes_map), \"base\": \"C\", \"use_cgan\": True},\n",
    "}\n",
    "\n",
    "\n",
    "def resolve_variant(key: str, use_cgan_balance: bool | None):\n",
    "    key = key.upper()\n",
    "    if key not in train_frames:\n",
    "        raise ValueError(f\"Modelo invalido: {key}. Use A, B, C, D ou E.\")\n",
    "    cfg = train_frames[key]\n",
    "    base_key = cfg[\"base\"]\n",
    "    num_classes = cfg[\"num_classes\"]\n",
    "    use_cgan = cfg[\"use_cgan\"] if use_cgan_balance is None else use_cgan_balance\n",
    "    return base_key, num_classes, use_cgan\n",
    "\n",
    "\n",
    "def build_train_df(base_df: pd.DataFrame, base_key: str):\n",
    "    base_key = base_key.upper()\n",
    "    if base_key == \"A\":\n",
    "        df = base_df.assign(label=base_df[\"class_name\"].apply(lambda c: 0 if c == \"No-Anomaly\" else 1))\n",
    "    elif base_key == \"B\":\n",
    "        df = base_df[base_df[\"class_name\"] != \"No-Anomaly\"].assign(label=lambda d: d[\"class_name\"].map(classes_map_B))\n",
    "    elif base_key == \"C\":\n",
    "        df = base_df.assign(label=lambda d: d[\"class_name\"].map(classes_map))\n",
    "    else:\n",
    "        raise ValueError(f\"Base invalida: {base_key}.\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_base_df(use_cgan_balance: bool, target_per_class=None):\n",
    "    if not use_cgan_balance:\n",
    "        return train_df\n",
    "    if \"balance_with_cgan\" not in globals():\n",
    "        raise NameError(\"balance_with_cgan nao definido. Corre a celula da Parte C primeiro.\")\n",
    "    return balance_with_cgan(target_per_class=target_per_class)\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model_name: str,\n",
    "    base_df: pd.DataFrame,\n",
    "    num_classes: int,\n",
    "    epochs: int = 3,\n",
    "    lr: float = 1e-3,\n",
    "    use_weighted_sampler: bool = True,\n",
    "    batch_size: int = 128,\n",
    "):\n",
    "    train_split, val_split = train_test_split(\n",
    "        base_df,\n",
    "        test_size=0.2,\n",
    "        stratify=base_df[\"label\"],\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    sampler = make_weighted_sampler(train_split) if use_weighted_sampler else None\n",
    "    train_loader = make_loader(\n",
    "        train_split,\n",
    "        train_transform,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=not use_weighted_sampler,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    val_loader = make_loader(val_split, test_transform, batch_size=batch_size)\n",
    "\n",
    "    model = NetworkCNN(num_classes).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(images), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * labels.size(0)\n",
    "        val_acc = evaluate_model(model, val_loader)\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs} | loss={running_loss / len(train_loader.dataset):.4f} | val_acc={val_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "    out_path = MODELS_DIR / f\"{model_name}.pth\"\n",
    "    torch.save(model.state_dict(), out_path)\n",
    "    print(f\"Modelo guardado em {out_path}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model_variant(key: str, use_cgan_balance: bool | None = None, target_per_class=None, **kwargs):\n",
    "    base_key, num_classes, use_cgan = resolve_variant(key, use_cgan_balance)\n",
    "    base_df = get_base_df(use_cgan, target_per_class=target_per_class)\n",
    "    df = build_train_df(base_df, base_key)\n",
    "    name = f\"model_{key}_scratch\"\n",
    "    return train_model(name, df, num_classes=num_classes, **kwargs)\n",
    "\n",
    "\n",
    "def run_kfold_cv(\n",
    "    model_name: str,\n",
    "    base_df: pd.DataFrame,\n",
    "    num_classes: int,\n",
    "    n_splits: int = 5,\n",
    "    epochs: int = 3,\n",
    "    batch_size: int = 128,\n",
    "    lr: float = 1e-3,\n",
    "    use_weighted_sampler: bool = True,\n",
    "):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    fold_rows = []\n",
    "\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(base_df, base_df[\"label\"]), start=1):\n",
    "        print(f\"\\nFold {fold_idx}/{n_splits}\")\n",
    "        train_split = base_df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_split = base_df.iloc[val_idx].reset_index(drop=True)\n",
    "        sampler = make_weighted_sampler(train_split) if use_weighted_sampler else None\n",
    "\n",
    "        train_loader = make_loader(\n",
    "            train_split,\n",
    "            train_transform,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=not use_weighted_sampler,\n",
    "            sampler=sampler,\n",
    "        )\n",
    "        val_loader = make_loader(val_split, test_transform, batch_size=batch_size)\n",
    "\n",
    "        model = NetworkCNN(num_classes).to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(images), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * labels.size(0)\n",
    "            val_acc = evaluate_model(model, val_loader)\n",
    "            print(\n",
    "                f\"  Epoch {epoch + 1}/{epochs} | loss={running_loss / len(train_loader.dataset):.4f} | val_acc={val_acc:.3f}\"\n",
    "            )\n",
    "\n",
    "        metrics = evaluate_model_metrics(model, val_loader)\n",
    "        fold_rows.append({\"fold\": fold_idx, **metrics})\n",
    "        print(\n",
    "            f\"Fold {fold_idx} metrics: acc={metrics['accuracy']:.3f} | f1_macro={metrics['f1_macro']:.3f} | f1_weighted={metrics['f1_weighted']:.3f}\"\n",
    "        )\n",
    "\n",
    "    fold_df = pd.DataFrame(fold_rows)\n",
    "    metrics_path = METRICS_DIR / f\"kfold_results_{model_name}.csv\"\n",
    "    METRICS_DIR.mkdir(exist_ok=True)\n",
    "    fold_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"Metricas K-fold guardadas em {metrics_path}\")\n",
    "    display(fold_df)\n",
    "    return fold_df\n",
    "\n",
    "\n",
    "def run_kfold_variant(key: str, use_cgan_balance: bool | None = None, target_per_class=None, **kwargs):\n",
    "    base_key, num_classes, use_cgan = resolve_variant(key, use_cgan_balance)\n",
    "    base_df = get_base_df(use_cgan, target_per_class=target_per_class)\n",
    "    df = build_train_df(base_df, base_key)\n",
    "    return run_kfold_cv(f\"{key}\", df, num_classes=num_classes, **kwargs)\n",
    "\n",
    "\n",
    "def run_kfold_all(keys=(\"A\", \"B\", \"C\", \"D\", \"E\"), target_per_class=None, **kwargs):\n",
    "    summaries = {}\n",
    "    for key in keys:\n",
    "        print(f\"\\n===== Modelo {key} =====\")\n",
    "        _, _, use_cgan_default = resolve_variant(key, use_cgan_balance=None)\n",
    "        summaries[key] = run_kfold_variant(key, use_cgan_balance=use_cgan_default, target_per_class=target_per_class, **kwargs)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "# Exemplos (comentados):\n",
    "# balanced_train_df = get_base_df(use_cgan_balance=True)  # gera e mistura reais+GAN\n",
    "# run_kfold_variant(\"A\", n_splits=5, epochs=3)            # Binario: real\n",
    "# run_kfold_variant(\"D\", n_splits=5, epochs=3)            # 11 classes anomalas: real + CGAN\n",
    "# run_kfold_variant(\"B\", n_splits=5, epochs=3)            # 11 classes anomalas: real\n",
    "# run_kfold_variant(\"C\", n_splits=5, epochs=3)            # 12 classes: real\n",
    "# run_kfold_variant(\"E\", n_splits=5, epochs=3)            # 12 classes: real + CGAN\n",
    "# train_model_variant(\"E\", epochs=5)                      # Treino rapido modelo E com dados mix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push rapido dos artefactos (usa HTTPS). Configura antes: git config user.email/name e token de acesso se precisa.\n",
    "# Descomenta as linhas abaixo quando estiveres autenticado.\n",
    "# !git status\n",
    "# !git add models/*.pth metrics/*.csv\n",
    "# !git commit -m \"Add modelos e metricas atualizadas\"\n",
    "# !git push origin main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anexo - GAN (opcional)\n",
    "Exemplo de como treinar um GAN condicional simples com o mesmo dataset (sem augmentations de flip/rotacao). Ajusta `num_epochs` e `batch_size` conforme recursos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN condicional simples (G e D totalmente conectados) para gerar imagens 64x64 em tons de cinza\n",
    "LATENT_DIM = 128\n",
    "NUM_CLASSES = len(classes_map)\n",
    "GAN_OUT_DIR = REPO_ROOT / \"cgan_generated_outputs\"\n",
    "GAN_OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + num_classes, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(1024, 1 * 64 * 64),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        x = torch.cat((self.label_emb(labels), noise), dim=1)\n",
    "        out = self.model(x)\n",
    "        return out.view(noise.size(0), 1, 64, 64)\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1 * 64 * 64 + num_classes, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        x = torch.cat((img.view(img.size(0), -1), self.label_emb(labels)), dim=1)\n",
    "        validity = self.model(x)\n",
    "        return validity\n",
    "\n",
    "def make_gan_loader(df, batch_size=64):\n",
    "    return DataLoader(\n",
    "        SolarDataset(df, train_transform),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "def train_cgan(train_df: pd.DataFrame, num_epochs: int = 5, batch_size: int = 64, lr: float = 2e-4, sample_every: int = 1):\n",
    "    loader = make_gan_loader(train_df, batch_size=batch_size)\n",
    "    generator = ConditionalGenerator(LATENT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "    discriminator = ConditionalDiscriminator(NUM_CLASSES).to(DEVICE)\n",
    "    adversarial_loss = nn.BCELoss()\n",
    "\n",
    "    opt_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(NUM_CLASSES, LATENT_DIM, device=DEVICE)\n",
    "    fixed_labels = torch.arange(NUM_CLASSES, device=DEVICE)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for imgs, labels in loader:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            valid = torch.ones(imgs.size(0), 1, device=DEVICE)\n",
    "            fake = torch.zeros(imgs.size(0), 1, device=DEVICE)\n",
    "\n",
    "            # Train generator\n",
    "            opt_G.zero_grad()\n",
    "            z = torch.randn(imgs.size(0), LATENT_DIM, device=DEVICE)\n",
    "            gen_labels = labels\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs, gen_labels), valid)\n",
    "            g_loss.backward()\n",
    "            opt_G.step()\n",
    "\n",
    "            # Train discriminator\n",
    "            opt_D.zero_grad()\n",
    "            real_loss = adversarial_loss(discriminator(imgs, labels), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), gen_labels), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | D loss: {d_loss.item():.4f} | G loss: {g_loss.item():.4f}\")\n",
    "\n",
    "        if (epoch + 1) % sample_every == 0:\n",
    "            with torch.no_grad():\n",
    "                samples = generator(fixed_noise, fixed_labels).cpu()\n",
    "                grid = vutils.make_grid(samples, nrow=4, normalize=True)\n",
    "                out_path = GAN_OUT_DIR / f\"cgan_samples_epoch_{epoch+1:03}.png\"\n",
    "                vutils.save_image(grid, out_path)\n",
    "                print(f\"Samples guardados em {out_path}\")\n",
    "\n",
    "    torch.save(generator.state_dict(), MODELS_DIR / \"cgan_generator.pth\")\n",
    "    print(\"Gerador guardado em models/cgan_generator.pth\")\n",
    "    return generator\n",
    "\n",
    "# Exemplo de uso (comentado): treinar GAN condicional usando o split de treino completo\n",
    "# generator = train_cgan(train_df, num_epochs=5, batch_size=128, sample_every=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6f8ca",
   "metadata": {},
   "source": [
    "### Parte C - CGAN para classes minoritarias\n",
    "Usa o gerador condicionado treinado para sintetizar imagens das classes com menos exemplos, guardar no disco e devolver um DataFrame balanceado (train + sinteticas). Depois podes treinar com `WeightedRandomSampler` para manter o balanceamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar imagens sinteticas para classes minoritarias com CGAN treinado\n",
    "CGAN_WEIGHTS = REPO_ROOT / \"cgan_generated_outputs\" / \"cgan_generator_minority_classes.pth\"\n",
    "GAN_AUG_DIR = DATA_DIR / \"cgan_augmented\"\n",
    "GAN_AUG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def load_trained_generator(weights_path=CGAN_WEIGHTS):\n",
    "    if not weights_path.exists():\n",
    "        raise FileNotFoundError(f\"Nao encontrei pesos do CGAN em {weights_path}. Treina o CGAN na celula anterior ou ajusta o caminho.\")\n",
    "    generator = ConditionalGenerator(LATENT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "    state = torch.load(weights_path, map_location=DEVICE)\n",
    "    generator.load_state_dict(state)\n",
    "    generator.eval()\n",
    "    return generator\n",
    "\n",
    "\n",
    "def generate_class_samples(generator, class_id: int, num_images: int, batch_size: int = 64):\n",
    "    saved_paths = []\n",
    "    remaining = num_images\n",
    "    while remaining > 0:\n",
    "        cur = min(batch_size, remaining)\n",
    "        noise = torch.randn(cur, LATENT_DIM, device=DEVICE)\n",
    "        labels = torch.full((cur,), class_id, device=DEVICE, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            imgs = generator(noise, labels).cpu()\n",
    "        for i in range(cur):\n",
    "            out_name = f\"cgan_{class_id:02d}_{len(saved_paths)+1:05d}.png\"\n",
    "            out_path = GAN_AUG_DIR / out_name\n",
    "            vutils.save_image(imgs[i], out_path, normalize=True)\n",
    "            saved_paths.append(out_path)\n",
    "        remaining -= cur\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "def balance_with_cgan(target_per_class: int | None = None, batch_size: int = 64):\n",
    "    \"\"\"Gera imagens sinteticas para cada classe ate target_per_class (default = max count atual).\"\"\"\n",
    "    generator = load_trained_generator()\n",
    "    class_counts = train_df[\"label\"].value_counts().to_dict()\n",
    "    if target_per_class is None:\n",
    "        target_per_class = max(class_counts.values())\n",
    "    new_rows = []\n",
    "    for cls_name, cls_id in classes_map.items():\n",
    "        cur = class_counts.get(cls_id, 0)\n",
    "        if cur >= target_per_class:\n",
    "            continue\n",
    "        need = target_per_class - cur\n",
    "        print(f\"Classe {cls_name} (id={cls_id}): gerar {need} para equilibrar {cur}->{target_per_class}\")\n",
    "        paths = generate_class_samples(generator, cls_id, need, batch_size=batch_size)\n",
    "        for p in paths:\n",
    "            new_rows.append({\"path\": p, \"class_name\": cls_name, \"label\": cls_id})\n",
    "    if not new_rows:\n",
    "        print(\"Dataset ja esta balanceado; nada gerado.\")\n",
    "        return train_df\n",
    "    synth_df = pd.DataFrame(new_rows)\n",
    "    balanced_df = pd.concat([train_df, synth_df], ignore_index=True)\n",
    "    print(f\"Geradas {len(synth_df)} imagens sinteticas. Total final: {len(balanced_df)}\")\n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "# Exemplo (comentado): balancear para o maximo atual por classe e usar WeightedRandomSampler depois\n",
    "# balanced_train_df = balance_with_cgan()\n",
    "# sampler = make_weighted_sampler(balanced_train_df)\n",
    "# loader = make_loader(balanced_train_df, train_transform, batch_size=128, sampler=sampler)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
